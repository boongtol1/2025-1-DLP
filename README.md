# 2025-1-DLP

# 딥러닝의 큰 그림과 핵심 직관

## 🧠 핵심 아이디어  
> **"딥러닝이란 태생적으로 잘난 부분을 강화해주는 과정이다."**

딥러닝은 손실 함수(Loss Function)의 값을 줄이기 위해, 여러 가중치들 중 손실에 민감한 것부터 조정해나가는 **경사하강법(Gradient Descent)** 기반의 최적화 과정이다.  
이때 **합성함수의 미분(Chain Rule)**을 이용해 **역전파(Backpropagation)**로 가중치들을 업데이트한다.

이 과정을 한 문장으로 압축하면 다음과 같다:

> **"태생적으로 잘난 부분을 강화해준다."**

---

## 🏗️ Dense Layer에서 이 문장이 성립하지 않는 이유

**Dense Layer (완전연결층)**는 모든 입력 뉴런이 다음 레이어의 모든 뉴런과 연결되는 구조다. 이로 인해:

- **초기에는 모든 입력이 동일한 조건**에서 시작한다.
- 학습 초기에는 어떤 feature가 "잘났다"거나 "덜 중요하다"고 구분할 수 없다.

즉, Dense Layer에서는 **태생적으로 잘난 부분을 논할 수 없다**.  
모든 feature는 처음에는 **동등하게 취급**되며, 학습을 통해 점차 **차이가 만들어진다.**

📌 정리:
> Dense Layer는 **학습을 통해 잘난 부분을 만들어가는 구조**이지, 처음부터 어떤 입력이 잘났다고 가정할 수는 없다.

---

## 🧠 CNN Layer에서 이 문장이 성립하는 이유

**CNN Layer (합성곱층)**에서는 상황이 다르다.

- 필터(커널)는 공간적으로 제한된 영역을 관찰함
- 어떤 필터는 초기에 **우연히 유의미한 시각 패턴**(예: 엣지, 곡선, 눈 등)을 감지하게 됨
- 이때 해당 필터가 출력에 크게 기여하면 **gradient가 커지고**, 그 결과 필터가 **더 예리해진다**

즉, CNN에서는 초기부터 **태생적으로 잘난 필터**가 생길 수 있으며, 학습을 통해 그 잘난 필터가 **더 강화**되는 현상이 자연스럽게 일어난다.

📌 이것이 바로:
> **"태생적으로 잘난 부분을 강화해준다"는 문장이 CNN의 중간 과정 사고로서 핵심적**인 이유다.

---

## 📊 Dense vs CNN 요약 비교

| Layer 종류    | "잘난 부분" 개념 | 학습의 방향성 |
|---------------|------------------|----------------|
| Dense Layer   | 존재하지 않음 (모든 입력은 동등) | 잘난 부분을 학습으로 "만들어간다" |
| CNN Layer     | 초기부터 존재 가능 | 잘난 필터를 점점 "강화해간다" |

---

## 🧩 철학적 관점

CNN 구조에서 보이는 이 현상은 단순히 기술적인 설명을 넘어 딥러닝의 **진화론적 메커니즘**과도 맞닿아 있다.

> **"무작위 속에서 우연히 잘난 존재가 나타나고, 그것이 환경(손실 함수)에 의해 선택되고 강화된다."**

이는 곧 딥러닝이 **경쟁과 적응을 통해 구조를 진화시키는 시스템**이라는 걸 보여준다.

---

## ✍️ 만든 사람
- Seung Hoon  
- 2025년 1학기 동양미래대학교 딥러닝프로그래밍 스터디
